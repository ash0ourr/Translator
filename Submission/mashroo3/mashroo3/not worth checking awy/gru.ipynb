{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70234bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, GRU, Dense, Embedding\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcaba62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'data/eng_-french.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_data(data):\n",
    "    data.rename(columns={\"English words/sentences\":\"Eng\", \"French words/sentences\":\"Frn\"}, inplace=True)\n",
    "    return data\n",
    "\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Tokenization and Padding\n",
    "def tokenize_and_pad(texts, max_len=None):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, padding='post', maxlen=max_len)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "eng_sequences, eng_tokenizer = tokenize_and_pad(data['Eng'])\n",
    "frn_sequences, frn_tokenizer = tokenize_and_pad(data['Frn'])\n",
    "\n",
    "eng_max_len = max(len(seq) for seq in eng_sequences)\n",
    "frn_max_len = max(len(seq) for seq in frn_sequences)\n",
    "\n",
    "eng_sequences, eng_tokenizer = tokenize_and_pad(data['Eng'], max_len=eng_max_len)\n",
    "frn_sequences, frn_tokenizer = tokenize_and_pad(data['Frn'], max_len=frn_max_len)\n",
    "\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "frn_vocab_size = len(frn_tokenizer.word_index) + 1\n",
    "\n",
    "train_eng, test_eng, train_frn, test_frn = train_test_split(eng_sequences, frn_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad848cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Seq2Seq Model with GRU\n",
    "def define_seq2seq_model(eng_vocab_size, frn_vocab_size, eng_max_len, frn_max_len, embedding_dim=256, gru_units=256):\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(eng_max_len,))\n",
    "    encoder_embedding = Embedding(eng_vocab_size, embedding_dim, input_length=eng_max_len, mask_zero=True)(encoder_inputs)\n",
    "    encoder_gru = GRU(gru_units, return_state=True)\n",
    "    _, state_h = encoder_gru(encoder_embedding)\n",
    "    encoder_states = [state_h]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(frn_max_len,))\n",
    "    decoder_embedding = Embedding(frn_vocab_size, embedding_dim, input_length=frn_max_len, mask_zero=True)(decoder_inputs)\n",
    "    decoder_gru = GRU(gru_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _ = decoder_gru(decoder_embedding, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(frn_vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf487ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 256\n",
    "gru_units = 256\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = define_seq2seq_model(eng_vocab_size, frn_vocab_size, eng_max_len, frn_max_len, embedding_dim, gru_units)\n",
    "model.compile(optimizer=Adam(learning_rate), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Training the model\n",
    "train_frn_input = train_frn[:, :-1]  # input to the decoder\n",
    "train_frn_output = train_frn[:, 1:]  # expected output of the decoder (shifted by one timestep)\n",
    "\n",
    "# Padding train_frn_input to ensure it matches the required input shape\n",
    "train_frn_input = pad_sequences(train_frn_input, maxlen=frn_max_len, padding='post')\n",
    "train_frn_output = pad_sequences(train_frn_output, maxlen=frn_max_len, padding='post')\n",
    "\n",
    "history = model.fit([train_eng, train_frn_input], train_frn_output,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85543a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English words/sentences    0\n",
       "French words/sentences     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the decoder model\n",
    "decoder_state_input_h = Input(shape=(gru_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h]\n",
    "\n",
    "decoder_outputs, state_h = decoder_gru(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f676326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate using the trained model\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with only the start character.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    \n",
    "    # Start character is assumed to be index 1 (usually 0 is reserved for padding)\n",
    "    target_seq[0, 0] = 1\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = frn_tokenizer.index_word.get(sampled_token_index, '')\n",
    "\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        if (sampled_word == '' or\n",
    "           len(decoded_sentence.split()) >= frn_max_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        states_value = [h]\n",
    "\n",
    "    return decoded_sentence.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eed4f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Random English - French Sentence --- \n",
      "English Sentence/Word:  I know Tom has gone. \n",
      "French Sentence/Word:  Je sais que Tom a disparu.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    input_seq = test_eng[i:i+1]\n",
    "    translated_sentence = decode_sequence(input_seq)\n",
    "    print('Input:', data['Eng'].iloc[i])\n",
    "    print('Translation:', translated_sentence)\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
